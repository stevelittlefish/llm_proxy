{
  "server": {
    "host": "0.0.0.0",
    "port": 11434,
    "enable_cors": true,
    "log_messages": false,
    "log_raw_requests": false,
    "log_raw_responses": false
  },
  "backend": {
    "type": "openai",
    "endpoint": "http://host.docker.internal:8008",
    "timeout": 300
  },
  "database": {
    "path": "/data/llm_proxy.db",
    "max_requests": 100,
    "cleanup_interval": 5
  }
}
