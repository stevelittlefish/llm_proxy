{
  "server": {
    "host": "0.0.0.0",
    "port": 11434,
    "enable_cors": true,
    "log_messages": false,
    "log_raw_requests": false,
    "log_raw_responses": false
  },
  "backend": {
    "type": "openai",
    "endpoint": "http://host.docker.internal:8008",
    "timeout": 300
  },
  "database": {
    "path": "/app/data/llm_proxy.db"
  },
  "models": {
    "default": "llama2",
    "mappings": {
      "llama2": "llama-2-7b-chat",
      "codellama": "codellama-7b"
    }
  }
}
