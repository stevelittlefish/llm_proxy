[server]
host = "0.0.0.0"
port = 11434
enable_cors = false
log_messages = true
log_raw_requests = false
log_raw_responses = false

[backend]
type = "openai"
endpoint = "http://localhost:8008"
timeout = 300
tool_blacklist = []  # List of tool names to filter out from requests, e.g., ["tool1", "tool2"]

[backend_openai]
force_prompt_cache = false

[database]
path = "./data/llm_proxy.db"
max_requests = 100
cleanup_interval = 5

[chat_text_injection]
enabled = false
text = "/nothink"
mode = "last"
