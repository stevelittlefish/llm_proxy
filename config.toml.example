[server]
host = "0.0.0.0"
port = 11434
enable_cors = false
log_messages = true
log_raw_requests = false
log_raw_responses = false

[backend]
type = "openai"
endpoint = "http://localhost:8008"
timeout = 300
tool_blacklist = []

[backend_openai]
force_prompt_cache = false

[database]
path = "./data/llm_proxy.db"
max_requests = 100
cleanup_interval = 5

[chat_text_injection]
enabled = false
text = "/nothink"
mode = "last"  # Options: "first", "last", or "system"
               # "first" - inject into first user message
               # "last" - inject into last user message
               # "system" - inject into system prompt (creates one if it doesn't exist)
