[server]
host = "0.0.0.0"
port = 11434
enable_cors = true
log_messages = false
log_raw_requests = false
log_raw_responses = false

[backend]
type = "openai"
endpoint = "http://host.docker.internal:8008"
timeout = 300

[backend_openai]
force_prompt_cache = false

[database]
path = "/data/llm_proxy.db"
max_requests = 100
cleanup_interval = 5

[chat_text_injection]
enabled = false
text = "/nothink"
mode = "last"
